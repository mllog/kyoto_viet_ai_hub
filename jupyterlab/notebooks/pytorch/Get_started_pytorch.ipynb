{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bắt Đầu với PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch là gì?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch là một thư viện Python (Là một wrapper của Torch - Thư viện C)\n",
    "    + Thay thế cho các thư viện tính toán khoa học (scientific computing framework, như Numpy) có khả năng kết hợp với GPU\n",
    "    + Cung cấp nền tảng đơn giản cho việc nghiên cứu Deep Learning\n",
    "    \n",
    "- Những ai đang sử dụng PyTorch\n",
    "    + Trong công nghiệp: Facebook, Twitter, NVIDIA, SaleForce, ...\n",
    "    + Trong nghiên cứu: CMU, Stanford, ...\n",
    "    + Ngày càng được ưa chuộng và dùng nhiều hơn\n",
    "\n",
    "Tutorial này giới thiệu các cài đặt PyTorch và dùng PyTorch để xây dựng một ứng dụng Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt PyTorch\n",
    "\n",
    "Cách cài đặt PyTorch đơn giản nhất là sử dụng Anaconda Python. Chạy lệnh sau trong một môi trường đã tạo:\n",
    "\n",
    "`conda install pytorch torchvision -c pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch \"Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo chúng ta sẽ sử dụng PyTorch để xây dựng ứng dụng \"Hello World\" của Deep Learning - Xây dựng mạng neuron để nhận dạng chữ số viết tay từ tập dữ liệu MNIST (Một tập dữ liệu chuẩn trong ML, thị giác máy tính (Computer Vision)).\n",
    "\n",
    "Chúng ta có thể tải dữ tập dữ liệu này từ [MNIST Website](http://yann.lecun.com/exdb/mnist/) và load dữ liệu PyTorch. Tuy nhiên để đơn giản hoá qui trình cho những nhà nghiên cứu và phát triền, PyTorch đã đóng gói những tập dữ liệu này thành những dữ liệu chuẩn trong thư viện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms),\n",
    "    batch_size=64, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn code trên tạo 2 đối tượng DataLoader (train_loader, test_loader) để download dữ liệu theo batch (64 ảnh mỗi batch) từ dữ liệu 60000 ảnh MNIST và lưu vào file train và test. Ngoài ra gói torchvision còn giúp chúng ta tạo được những pipeline phức tạp để xử lý dữ liệu như cropping, xoay ảnh, scale ảnh, .... Trong ví dụ trên chúng ta load ảnh MNIST (28x28 pixels) chuyển thành tensor (1x28x28) sau đấy thì chuẩn hoá giá trị pixels từ giá trị từ 0...255 về giá trị từ -1...1 băng cách định nghĩa lớp Normalize với mean (0.1307) và standard deviation (0.3081) của toàn bộ tập dữ liệu MNIST. Việc chuẩn hoá dữ liệu sẽ giúp việc training mạng neural tốt hơn rất nhiều."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Định nghĩa mạng Neuron\n",
    "Tiếp theo chúng ta sẽ tạo một mạng neural sử dụng Pytorch nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelloWorldNet(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super(HelloWorldNet, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.fc0 = nn.Linear(image_size, 1000)\n",
    "        self.fc1 = nn.Linear(1000, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.image_size)\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để xây dựng một mạng neural, chúng ta tạo một lớp kế thừa từ nn.Module và định nghĩa hàm khởi tạo và hàm `forward()`.\n",
    "\n",
    "Chúng ta định nghĩa tất cả các lớp của mạng neuron trong hàm khởi tạo, ở ví dụ trên chúng ta tạo một mạng neuron đơn giản với 3 fully-connected layers, sử dụng hàm tuyến tính. \n",
    "\n",
    "Hàm `forward()` định nghĩa cách tính toán giá trị đầu ra cho mỗi layer. Đầu tiên chúng ta phải đổi tensor của ảnh (1x28x28) thành một dang có chiều phù hợp cho lớp đầu tiên của mạng neuron. Hàm `view()` giúp chúng ta làm điều này. Trong ví dụ trên tensor sẽ được dàn (flattens) thành một tensor có chiều là 1x784. Các dòng tiếp theo định nghĩa \"activation function\" cho các layers, trong ví dụ chúng ta sử dụng ReLu (Rectified Linear Unit), ở lớp cuối cùng chúng ta dùng softmax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo chúng ta có thể tạo model bằng cách dùng lớp trên:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HelloWorldNet(image_size=28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chúng ta đang làm việc trên máy có GPU, chúng ta cho thể chuyển model sang chế độ chạy trên GPU bằng cách goi hàm `cuda()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloWorldNet(\n",
      "  (fc0): Linear(in_features=784, out_features=1000, bias=True)\n",
      "  (fc1): Linear(in_features=1000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi định nghĩa mạng neuron, chúng ta phải \"dạy\" model sử dụng dữ liệu học. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        data, labels = Variable(data), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Eporch: {} [{}/{} ({:.0f}%)\\t Loss: {:.6f}]'.format(epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                                                                            100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước khi chúng ta tạo hàm `train()` chúng ta phải khởi tạo một \"optimizer\", optimizer có nhiệm vụ điều chỉnh tham số (parameters) của mỗi layer ở mỗi bước chúng ta train model với một batch dữ liệu. Có nhiều thuật toán optimizer như [RMSProp, AdaGrad, Adam ...](http://ruder.io/optimizing-gradient-descent/index.html). Thuật toán tối ưu được dùng phổ biết nhất hiện này là Adam. Trong ví dụ trên chúng ta sử dụng một thuật toán đơn giản [Stochastic Gradient Descent](http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent).\n",
    "\n",
    "trong hàm `train()` chúng ta load dữ liệu ở mỗi batch từ tập dữ liệu học, model sẽ tính đâu ra và so sánh với giá trị thực tế thông qua hàm loss. Sau đấy chúng ta gọi một hàm mà người ta gọi là PyTorch magic, loss.backward(). Hàm này sẽ tự động tính \"backpropagation\" để đưa ra giá tị update cho tham số, sau cùng chúng ta điều chỉnh tham số thông qua hàm `step()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để đánh giá độ chính xác của model chúng ta tạo hàm `test` để tính độ chính xác bằng cách chạy model trên dữ liệu test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trang/anaconda/envs/jupyterlab/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eporch: 1 [0/60000 (0%)\t Loss: 2.308312]\n",
      "Train Eporch: 1 [6400/60000 (11%)\t Loss: 2.301066]\n",
      "Train Eporch: 1 [12800/60000 (21%)\t Loss: 2.250018]\n",
      "Train Eporch: 1 [19200/60000 (32%)\t Loss: 2.230992]\n",
      "Train Eporch: 1 [25600/60000 (43%)\t Loss: 2.251629]\n",
      "Train Eporch: 1 [32000/60000 (53%)\t Loss: 2.212543]\n",
      "Train Eporch: 1 [38400/60000 (64%)\t Loss: 2.110949]\n",
      "Train Eporch: 1 [44800/60000 (75%)\t Loss: 2.184118]\n",
      "Train Eporch: 1 [51200/60000 (85%)\t Loss: 2.130153]\n",
      "Train Eporch: 1 [57600/60000 (96%)\t Loss: 2.051203]\n",
      "\n",
      "Test set: Average loss: 2.0537, Accuracy: 3924/10000 (39%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1 + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có được độ chính xác trên tập test đạt khoảng 82% sau 10 epoches. Not bad! cho một model đơn giản."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dùng Convolutions Neural Network cho kết quả tốt hơn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eporch: 1 [0/60000 (0%)\t Loss: 2.311843]\n",
      "Train Eporch: 1 [6400/60000 (11%)\t Loss: 2.301671]\n",
      "Train Eporch: 1 [12800/60000 (21%)\t Loss: 2.290457]\n",
      "Train Eporch: 1 [19200/60000 (32%)\t Loss: 2.297308]\n",
      "Train Eporch: 1 [25600/60000 (43%)\t Loss: 2.274854]\n",
      "Train Eporch: 1 [32000/60000 (53%)\t Loss: 2.291910]\n",
      "Train Eporch: 1 [38400/60000 (64%)\t Loss: 2.267767]\n",
      "Train Eporch: 1 [44800/60000 (75%)\t Loss: 2.255762]\n",
      "Train Eporch: 1 [51200/60000 (85%)\t Loss: 2.256258]\n",
      "Train Eporch: 1 [57600/60000 (96%)\t Loss: 2.266452]\n",
      "\n",
      "Test set: Average loss: 2.2526, Accuracy: 3245/10000 (32%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNNNet()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "for epoch in range(1, 1 + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có được độ chính xác là 91%, tốt hơn khá nhiều so với HelloWord net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterCNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BetterCNNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(64*12*12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 64*12*12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eporch: 1 [0/60000 (0%)\t Loss: 2.287128]\n",
      "Train Eporch: 1 [6400/60000 (11%)\t Loss: 0.251294]\n",
      "Train Eporch: 1 [12800/60000 (21%)\t Loss: 0.067553]\n",
      "Train Eporch: 1 [19200/60000 (32%)\t Loss: 0.110949]\n",
      "Train Eporch: 1 [25600/60000 (43%)\t Loss: 0.019545]\n",
      "Train Eporch: 1 [32000/60000 (53%)\t Loss: 0.050329]\n",
      "Train Eporch: 1 [38400/60000 (64%)\t Loss: 0.034056]\n",
      "Train Eporch: 1 [44800/60000 (75%)\t Loss: 0.073841]\n",
      "Train Eporch: 1 [51200/60000 (85%)\t Loss: 0.204919]\n",
      "Train Eporch: 1 [57600/60000 (96%)\t Loss: 0.026304]\n",
      "\n",
      "Test set: Average loss: 0.0570, Accuracy: 9816/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BetterCNNNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "for epoch in range(1, 1 + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Học học nữa học mãi\n",
    "More example: http://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning by doing and earning\n",
    "Participate in Kaggle competitions: https://www.kaggle.com/competitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyterlab)",
   "language": "python",
   "name": "jupyterlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
